\listfiles

\documentclass[11.5pt,a4paper]{article}

\usepackage{amsthm,amssymb,amsmath}
\usepackage{wasysym}
\usepackage{mathtools}

\newcommand\persiangloss[2]{#1\dotfill\lr{#2}\\}

\newcommand{\nocontentsline}[3]{}
\newcommand{\tocless}[2]{\bgroup\let\addcontentsline=\nocontentsline#1{#2}\egroup}
\usepackage[bottom]{footmisc}
\usepackage{indentfirst}

\usepackage{caption}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}
\usepackage{adjustbox}
\usepackage{tablefootnote}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{yfonts}

\usepackage[scr=euler,bb=ams]{mathalfa}

\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.90}
\definecolor{LGray}{gray}{0.95}

\usepackage[pagebackref=false,colorlinks,linkcolor=blue,citecolor=magenta]{hyperref}

\usepackage[a4paper]{geometry} 
\geometry{a4paper,tmargin=3.5cm, bmargin=2.5cm, lmargin=2cm, rmargin=2.5cm, headheight=3em, headsep=1.5cm, footskip=1cm} 

\usepackage{xepersian}
\settextfont[Scale=1]{B Nazanin}
%\setlatintextfont[Scale=1]{Times New Roman}

%\settextfont[Scale=1.1]{B Zar}

%\DefaultMathsDigits
\setdigitfont{XB Zar}

\defpersianfont\titr[Scale=1]{B Titr}
%\defpersianfont\nastaliq[Scale=1.5]{IranNastaliq}
%\defpersianfont\traffic[Scale=1]{B Traffic}
%\defpersianfont\yekan[Scale=1]{B Yekan}
%\defpersianfont\traffic[Scale=1]{XB Roya}
%\defpersianfont\yekan[Scale=1]{XB Kayhan}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{zref-perpage}
\zmakeperpage{footnote}



\begin{document}

\thispagestyle{empty}
\vspace*{-28mm}
\centerline{\includegraphics[height=5cm]{/home/ahmad/Documents/Latex/Imgs/ceit_logo.png}}

\begin{center}
%دستوری برای کم کردن فاصله بین لوگو و خط پایین آن
\vspace{-2mm}
{\LARGE
{
دانشکده مهندسی کامپیوتر و فن‌آوری اطلاعات\\	
دانشگاه صنعتی امیرکبیر	
}
%دستوری برای تعیین فاصله بین دو خط
\\[2.1cm]
}

{\large
\textbf{گزارش تمرین اول درس مدل‌های احتمالاتی گرافی}
\\[2cm]

استاد درس:
\\[.5cm]
{\Large
دکتر نیک‌آبادی}
\\[1.5cm]
\large 
نام دانشجو:
\\[.5cm]
{\Large
احمد اسدی}
\\[.5cm]
۹۴۱۳۱۰۹۱
\\[1.5cm]
}
%دستوری برای تعیین فاصله بین خطوط (نه دو خط) و تا وقتی که مقدار آن تغییر نکند، فاصله بین خطوط، همین مقدار است.

{\large
فروردین ۱۳۹۵
}
\end{center}

\newpage
\baselineskip=1cm
\tocless\tableofcontents

\newpage
\baselineskip=0.75cm
\pagenumbering{arabic}
\section{پیش‌پردازش و استخراج ویژگی‌ها}
در این بخش به بررسی عملیات پیش‌پردازش و استخراج ویژگی‌ها ‌می‌پردازیم. تمامی مراحل مربوط به پیش‌پردازش اطلاعات و ویژگی‌های استخراج شده، شامل نحوه استخراج ویژگی‌ها، نحوه محاسبه توزیع‌های احتمالی شرطی\footnote{Conditional Probability Distribution (CPD)} و نحوه نگاشت داده‌ها به فضای برداری را در این قسمت بررسی خواهیم نمود.

\subsection{پیش‌پردازش}
قبل از بررسی عملیات مربوط به پیش‌پردازش داده‌ها،‌ ابتدا باید ساختار داده‌ها را شناخت. در مجموعه‌داده مورد استفاده\footnote{مجموعه‌داده 20 NewsGroups} هر فایل خبری شامل دو بخش اصلی است:
\begin{itemize}
\item [سرآیند] 
در این بخش، اطلاعات کلی شامل اطلاعاتی مانند شناسه فایل خبری، شناسه اخبار مرجع، رایانامه نویسنده خبر، نام نویسنده،‌ موضوع خبر،‌ نام سازمان مرتبط و مانند آن وجود دارد. از آنجا که بخش قابل توجهی از این اطلاعات مربوط به دسته‌خبری می‌باشند، نمی‌توان از آن‌ها در فرآیند دسته‌بندی استفاده نمود. با این حال، مواردی مانند نام نویسنده، رایانامه نویسنده، موضوع خبر، تعداد خطوط موجود در خبر و اطلاعاتی از این قبیل می‌توانند نماینده‌های خوبی به عنوان ویژگی باشند.
\item [بدنه اصلی]
در این بخش، محتوا و متن خبر وجود دارد. این محتوا در تمام فایل‌ها با یک خط خالی از بخش سرآیند تفکیک شده است. این بخش، در تمام فرآیند دسته‌بندی مورد استفاده قرار می‌گیرد و همان‌طور که در ادامه توضیح داده خواهد شد، تمام ویژگی‌های مربوطه از این بخش استخراج می‌شوند.
\end{itemize}

با توجه به ساختار داده‌ها و از آنجا که دقت نهایی الگوریتم در این تمرین حائز اهمیت نمی‌باشد،‌ از بخش سرآیند به طور کلی صرف نظر نموده و تمام عملیات‌های خود را روی بخش بدنه اصلی اخبار انجام می‌دهیم. در این بخش نیاز داریم تا کلمات معنادار اخبار را جدا نموده و از بین آن‌ها تعدادی را به عنوان ویژگی انتخاب کرده و مورد استفاده قرار دهیم. به همین منظور، عملیات پیش‌پردازش مناسب باید بتواند کلمات را از ارقام و علامت‌های نگارشی جدا نماید تا بتوان از بین آن‌ها کلمات کلیدی را با دقت بیشتری انتخاب نمود.\\
از این رو، در مرحله پیش‌پردازش، ابتدا کلمات هر فایل را با توجه به کاراکتر فاصله جدا می‌نماییم. در این پروژه، کلماتی را که شامل ارقام هستند به کلی حذف کرده و از آن‌ها استفاده نمی‌نماییم. جدول \ref{table:preprocessDigits} تعدادی از کلماتی را که در مجموعه‌داده وجود دارند و شامل اعداد هستند نمایش می‌دهد.


\begin{table}[h]
\center
\caption{نمونه‌هایی از کلماتی که در مجموعه‌داده وجود دارند و شامل ارقام هستند.}
\label{table:preprocessDigits}
\begin{tabular}{c | c | c | c}
کلمه & شماره فایل & دسته‌خبری & مجموعه‌داده\\
\hline
\hline
	\begin{latin}
	ISBN0-910309-26-4 
	\end{latin}
	& 49960 & 
	\begin{latin}
	alt.atheism 
	\end{latin}
	& 
	\begin{latin}
	20NewsGroups
	\end{latin}
\\
	\begin{latin}
	D-3000Hannover
	\end{latin}
	& 49960 & 
	\begin{latin}
	alt.atheism
	\end{latin}
	& 
	\begin{latin}
	20NewsGroups
	\end{latin}
\\
	\begin{latin}
	416-629-7000/629-7044
	\end{latin}
	&38489&
	\begin{latin}
	comp.graphics
	\end{latin}
	&
	\begin{latin}
	mini\_newsgroups
	\end{latin}
\\
	\begin{latin}
	S1/S2
	\end{latin}
	&15423&
	\begin{latin}
	sci.crypt
	\end{latin}
	&
	\begin{latin}
	mini\_newsgroups
	\end{latin}
\end{tabular}
\end{table}

در مرحله دوم پیش‌پردازش، تمام علائم نگارشی از محتوا حذف می‌شوند. علاوه بر این، ممکن است کلماتی در متن خبر وجود داشته باشند که شامل علائمی غیر از حروف انگلیسی باشند. در این موارد نیز تمام علامت‌های به غیر از حروف الفبا حذف شده و کلمه اصلی حفظ می‌شود. جدول \ref{table:preprocessPuncts}
نمایش‌دهنده نمونه‌هایی از کلامتی از این دست در کنار قالب پردازش‌شده آن‌ها می‌باشد.


\begin{table}[h]
\caption{نمونه‌هایی از پیش‌پردازش کلماتی که شامل علامت‌های نگارشی هستند.}
\label{table:preprocessPuncts}
\centering
\begin{tabular}{c | c}
کلمه اصلی & نتیجه پیش‌پردازش\\
\hline
\hline
	\begin{latin}
	joke>
	\end{latin}
	&
	\begin{latin}
	joke
	\end{latin}
\\
	\begin{latin}
	><disclaimer:
	\end{latin}
	&
	\begin{latin}
	disclaimer
	\end{latin}
\\
\end{tabular}
\end{table}

مرحله دیگری که در بخش پیش‌پردازش انجام می‌شود، حذف کلمات توقف \footnote{Stop Words} است. در این مورد،‌از آنجا که این کلمات در تمام دسته‌های خبری به طور یکنواخت تکرار می‌شوند و کمکی به دسته‌بندی صحیح نمی‌کنند،‌برای کاهش حجم محاسبات،‌آن‌ها را در این مرحله حذف می‌کنیم. در صورتی‌که این کلمات در این مرحله حذف نشوند،‌تاثیری در عملکرد بخش استخراج ویژگی نخواهند داشت؛ زیرا در مرحله استخراج ویژگی‌ها،‌ کلماتی را که به طور یکنواخت در دسته‌های خبری تکرار شده‌اند به عنوان ویژگی،‌انتخاب نمی‌کنیم. این مرحله فقط برای کاهش حجم پردازش‌ها می‌باشد.

\subsection{استخراج ویژگی‌ها}
پس از مرحله پیش‌پردازش، لیستی از کلمات موجود در مجموعه‌داده بدست می‌آوریم. به منظور دست‌یابی به ویژگی‌های مناسب، یک شاخص\footnote{Index} از تعداد حضور هر ویژگی در هر دسته‌خبری تولید می‌نماییم. این شاخص در قالب یک ماتریس بیان می‌شود که ستون‌های آن، کلمات بدست‌آمده در مرحله قبل و سطرهای آن، دسته‌های خبری موجود را نمایندگی می‌کنند. در هر سلول از این داده‌ساختار، تعداد حضور کلمه مربوطه بین تمام ‌‌داده‌های دسته خبری متناظر آن قرار می‌گیرد. \\
شاخص تولید شده علاوه بر این‌که برای انتخاب ویژگی استفاده می‌شود، خود به نوعی نمایش‌دهنده توزیع احتمال توام هر ویژگی و دسته‌های خبری می‌باشد. رابطه \ref{eq:indexMatrix} فرم کلی شاخص را نمایش می‌دهد. در این ماتریس  $k$ تعداد دسته‌های خبری موجود و $n$ تعداد کل کلمات می‌باشد.
 $W_{ij}$ نمایش‌دهنده تعداد تکرار کلمه $j$ در میان اخبار موجود در دسته‌خبری $i$ است.


\begin{equation}
\label{eq:indexMatrix}
W = \begin{bmatrix}
W_{11} & W_{12} & \cdots & W_{1n} \\
W_{21} & W_{22} & \cdots & W_{2n} \\
\cdots & \cdots & \cdots & \cdots \\
W_{k1} & W_{k2} & \cdots & W_{kn} \\
\end{bmatrix}
\end{equation}

با محاسبه ماتریس شاخص به شکل روبرو، در هر سطر از ماتریس می‌توان کلماتی را که در یک دسته‌خبری بیشتر از بقیه کلمات تکرار شده‌اند را بدست آورد. از طرفی در هر ستون این ماتریس،‌ می‌توان وضعیت تکرار کلمه مشخصی را در دسته‌های خبری مختلف مشاهده نمود. اگر کلمه‌ای در یکی از دسته‌های خبری تعداد تکرار به مراتب بیشتری نسبت به کلمه‌های دیگر داشته باشد، آن کلمه را به عنوان یکی از ویژگی‌ها انتخاب می‌کنیم. با این توضیح، کافیست ماتریس شاخص را به صورت ستونی پیمایش نماییم و اگر در یک ستون از ماتریس، جهش قابل مشاهده‌ای در مقدار یک سلول وجود داشت، کلمه متناظر آن ستون را به عنوان یکی از ویژگی‌ها برمی‌گزینیم.\\
علاوه بر این، اگر مقدار موجود در هر خانه ماتریس را به مجموع مقادیر هم ستون آن خانه تقسیم نماییم، احتمال رخ‌داد توام کلمه مربوطه را در دسته‌خبری متناظر بدست می‌آوریم. رابطه \ref{eq:PXC} این عملیات را نمایش می‌دهد. از آنجا که در این رابطه، تعداد تکرار کلمه $i$ در دسته خبری $j$ به مجموع تعداد تکرار این کلمه در تمام دسته‌های خبری تقسیم شده است، این رابطه احتمال وجود کلمه $i$ را در دسته‌خبری $j$ محاسبه می‌نماید. با توجه به رابطه \ref{eq:PXCProof} مجموع احتمال‌ حضور کلمه $i$ در تمام دسته‌های خبری برابر یک بوده و بنابر این، رابطه احتمال \ref{eq:PXC} یک توزیع احتمال معتبر را نمایش می‌دهد.

\begin{equation}
\label{eq:PXC}
P(X_i, C_j) = \frac{W_{ij}}{\Sigma_{h=1}^{k} W_{ih}}
\end{equation}
\begin{equation}
\label{eq:PXCProof}
\Sigma_{l=1}^{k} P(X_i, C_l) = \Sigma_{l=1}^{k} \frac{W_{il}}{\Sigma_{h=1}^{k} W_{ih}} = \frac{\Sigma_{l=1}^{k} W_{il}}{\Sigma_{h=1}^{k} W_{ih}} = 1 
\end{equation}

در حالت برداری، می‌توان این عملیات را به شکل زیر انجام داد. در رابطه \ref{eq:vecPXC}، $sum(W)$ برداری است که هر سلول آن مجموع مقادیر ستون‌ متناظر در ماتریس $W$ را نمایش می‌دهد.

\begin{equation}
\label{eq:vecPXC}
W = \frac{W}{sum(W)}
\end{equation}

همان‌طور که گفتیم، کلماتی که بیشتر در یکی از دسته‌های خبری تکرار شوند و در دسته‌های دیگر کمتر دیده شوند،‌ انتخاب‌های مناسبی به عنوان ویژگی هستند. به همین دلیل پس از محاسبه احتمال حضور کلمات در دسته‌های خبری مختلف، مطابق با رابطه \ref{eq:selectFeats} کلماتی که احتمال حضور آن‌ها بیشتر از $\alpha$ درصد از احتمال توزیع یکنواخت بین دسته‌های خبری باشد به عنوان ویژگی انتخاب می‌شود.

\begin{equation}
\label{eq:selectFeats}
\Omega = \{X_i | (1 + \alpha) \cdot min(W_{1:n,1:k}) < P(X_i , C_j)\}
\end{equation}

استفاده از رابطه \ref{eq:selectFeats}، به ما این امکان را می‌دهد که با تغییر دادن مقدار $\alpha$، تعداد ویژگی‌های انتخاب شده را تغییر دهیم. اگر $\alpha = 0 $ انتخاب شود، تمام کلمات متن، به عنوان ویژگی انتخاب می‌شوند که بیشترین تعداد ممکن است. همین‌طور اگر $\alpha = \frac{max(W_{1:n,1:k})}{min(W_{1:n,1:k})}-1$ انتخاب شود، فقط کلماتی که دارای بیشترین انحراف معیار هستند به عنوان ویژگی انتخاب می‌شوند و در صورتی‌ که $\alpha$ بزرگتر از این مقدار تعیین شود،‌هیچ کلمه‌ای به عنوان ویژگی انتخاب نخواهد شد. بنابراین، مقدار این پارامتر در آزمایش‌های مختلف تغییر می‌کند تا به بهترین مقدار ممکن آن دست پیدا کنیم.
\section{ماتریس درهم‌ریختگی و تاثیر تعداد کلمات کلیدی منتخب از دسته‌های خبری مختلف}

جدول
\ref{tbl:2ccm}
 ماتریس درهم‌ریختگی را برای حالت دو کلاسه نمایش می‌دهد. در این آزمایش،‌ از مجموعه داده دوم \footnote{mini\_newsgroups} استفاده شده و تمام عملیات فقط به ازای داده‌های این دو کلاس انجام شده است. از هر کلاس، تعداد 100 داده در کل مورد استفاده قرار گرفته است که در مجموع 140 داده برای آموزش و 60 داده برای آزمایش به‌کار گرفته شده است. لازم به ذکر است در این آزمایش، مقدار $\alpha = 0.3$ انتخاب شده و تعداد ویژگی‌های بدست آمده برابر با 3674 ویژگی بوده است.
\begin{table}[h]
\center
\caption{ماتریس درهم‌ریختگی حالت دو کلاسه}
\label{tbl:2ccm}
\begin{tabular}{c | c | c | c | c}
& کلاس اول داده آزمایشی & کلاس دوم داده آزمایشی & کلاس اول داده آموزشی& کلاس دوم داده آموزشی\\
\hline
\hline
تعداد واقعی موجود&  27 &  33 &73&67 \\
درست مثبت \footnote{True Positive}&   26  &   19 &73&46 \\
غلط مثبت \footnote{False Positive}&  1  &  14 &21&0 \\
نرخ درست مثبت\footnote{True Positive Rate (TPR)} &  0.9630   &   0.5758  &1&0.6866 \\
نرخ غلط مثبت\footnote{False Positive Rate (FPR)} &  0.5185   &   0.0303 &0.2876&0 \\

\end{tabular}
\end{table}

جدول 
\ref{tbl:20ccm171tr}
ماتریس درهم‌ریختگی را برای حالت بیست کلاسه و با داده‌های آموزشی نمایش می‌دهد. در این آزمایش مقدار $\alpha = 0.3$ انتخاب شده و تعداد ویژگی‌های بدست آمده برابر با 171 ویژگی می‌باشد. همان‌طور که انتظار می‌رفت، از آنجا که تعداد کلاس‌ها در این حالت ۱۰ برابر حالت قبل است،‌ با در نظر گرفتن $\alpha = 0.3$ فقط ویژگی‌هایی انتخاب می‌شوند که تفاوت بسیار چشم‌گیری با سایر ویژگی‌های کاندید دارند. 
\\
\begin{table}[h]
\center
\caption{ماتریس درهم‌ریختگی حالت بیست کلاسه با داده‌های آموزشی و $\alpha = 0.3$}
\label{tbl:20ccm171tr}
\begin{tabular}{c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c }
  شماره کلاس & ۱ & ۲ & ۳ & ۴ & ۵ & ۶ & ۷ & ۸ & ۹ & ۱۰ & ۱۱ & ۱۲ & ۱۳ & ۱۴ & ۱۵ & ۱۶ & ۱۷ & ۱۸ & ۱۹ & ۲۰ \\
\hline
\hline
تعداد واقعی موجود& 63  & 69   &  70  &  72  &  68  &  70  &  74  &  77  &  69  &  68  & 68   &  74  &  65  &  67  &  72  &  70  &  75  &  68  &  72  &  69  \\
درست مثبت& 20  &  43  &  25  &  29  &  14  &  11  &  5  &  68  &  19  &  1  &  4  &  28  & 5   &  33  &  0  &  38  &  33  &  27  &  35  &  23  \\
غلط مثبت& 8  &  9  &  9  &  25  &  9  &  4  &  7  &  744  &  7  &  0  &  1  &  20  &  1  & 28   &  0  & 20  &  11  &  1  &  28  &  7  \\
\end{tabular}
\end{table}

با توجه به نتایج این جدول می‌توان دریافت، تعداد کلمات کلیدی کلاس ۸ که به عنوان ویژگی انتخاب شده‌اند، تفاوت چشم‌گیری نسبت به بقیه ویژگی‌ها داشته‌اند. به همین دلیل، الگوریتم بیشتر پیش‌بینی‌هایش را برابر با کلاس ۸ انجام داده است. علاوه بر این،
تعداد کلمات کلیدی از دسته‌های‌ خبری 10، 15، 11، 7 و 13 که به عنوان ویژگی انتخاب شده‌اند برعکس دسته‌خبری ۸، کم است و الگوریتم به سختی قادر به تشخیص آن‌ها می‌باشد.
\\

جدول
\ref{tbl:20ccm171ts}
ماتریس درهم‌ریختگی را به ازای داده‌های آزمایشی نشان می‌دهد. تاثیر تعداد کلمات کلیدی انتخاب شده از هر دسته‌خبری در این ماتریس به خوبی مشاهده می‌شود. همان‌طور که قبلا ذکر شد، الگوریتم به سختی قادر به تشخیص داده‌های دسته‌خبری 10 و 15 می‌باشد. در این ماتریس، الگوریتم قادر به تشخیص هیچ‌کدام از داده‌های این دسته نشده است. علاوه بر این، نرخ درست مثبت الگوریتم در دسته‌خبری ۸ بیشترین مقدار خود را دارد و به 0.87 می‌رسد.

\begin{table}[h]
\center
\caption{ماتریس درهم‌ریختگی حالت بیست کلاسه با داده‌های آزمایشی و $\alpha = 0.3$}
\label{tbl:20ccm171ts}
\begin{tabular}{c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c }
  شماره کلاس & ۱ & ۲ & ۳ & ۴ & ۵ & ۶ & ۷ & ۸ & ۹ & ۱۰ & ۱۱ & ۱۲ & ۱۳ & ۱۴ & ۱۵ & ۱۶ & ۱۷ & ۱۸ & ۱۹ & ۲۰ \\
\hline
\hline
تعداد واقعی موجود& 37  &  31  &  30  &  28  &  32  &  30  &  26  &  23  &  31  &  32  & 32   &  26  &  35  &  33  &  28  &  30  &  25  &  32  &  28  &   31 \\
درست مثبت& 11  &  12  &  4  &  14  &  3  &  0  &  1  &  20  &  8  &  0  &  1  &  12  & 3   &  12  &  0  &  15  &  10  &   12 &  13  &  7  \\
غلط مثبت&  2 &  3  & 5   &  8  &  4  &  10  &  5  &  347  &  6  & 0   &  1  &  8  & 0   &   12 &  0  &  7  &  12  &  0  &  8  &  4  \\

\end{tabular}
\end{table}

\section{تاثیر تعداد ویژگی‌ها}
در این بخش، برای بررسی تاثیر تعداد ویژگی‌های انتخابی بر عملکرد الگوریتم،‌ نتیجه دسته‌بندی را با نتایج ارائه شده در بخش قبلی و با مقادیر مختلف برای $\alpha$ مقایسه خواهیم نمود. همان‌طور که قبلا ذکر شد،‌ هرچه مقدار $\alpha$ بزرگتر باشد، تعداد ویژگی‌های انتخاب شده کاهش می‌یابند. در جدول \ref{tbl:2ccm} بخش قبل، ماتریس درهم‌ریختگی برای حالت دو کلاسه نمایش داده شده است. در این جدول،‌ مقدار $\alpha = 0.3$ انتخاب شده است. برای حالت دو کلاسه،‌ این مقدار مناسب است. در جدول 
\ref{tbl:2ccm8}
نتیجه همان آزمایش، با مقدار $\alpha = 0.8$ که منجر به انتخاب 
.....
ویژگی می‌شود،
گزارش شده است. 

\begin{table}[h]
\center
\caption{ماتریس درهم‌ریختگی برای حالت دو کلاسه با $\alpha = 0.8$}
\label{tbl:2ccm8}
\begin{tabular}{c | c | c}
& کلاس اول & کلاس دوم
\\
\hline
\hline

تعداد واقعی & &
\\
درست مثبت &&
\\
غلط مثبت&&
\\
نرخ درست مثبت&&
\\
نرخ غلط مثبت&&
\\

\end{tabular}
\end{table}

همان‌طور که مشاهده می‌شود با کاهش تعداد ویژگی‌ها،‌ ..........
\\

علاوه بر این، در بخش قبل در جدول 
\ref{tbl:20ccm171tr} 
و جدول
\ref{tbl:20ccm171ts}
به ترتیب،‌ ماتریس‌های درهم‌ریختگی مرتبط با حالت بیست‌کلاسه و $\alpha = 0.3$ گزارش شده‌اند. مقدار $\alpha = 0.3 $ برای حالت بیست‌کلاسه مقداری بزرگ است؛ چون در این حالت، مقادیر احتمال‌های حضور کلمات در دسته‌های خبری کوچم و به هم نزدیک است. به همین دلیل،‌ بیشتر احتمال‌ها در یک بازه نزدیک به مقدار 0.05 قرار دارند. این مشکل باعث شده فقط 171 کلمه در محدوده قابل قبول قرار بگیرند و به عنوان ویژگی انتخاب شوند.\\
در این بخش با تعیین مقدار $\alpha = .......$ تعداد ویژگی‌های انتخاب شده را افزایش می‌دهیم. در این حالت چون تعداد ویژگی‌های انتخاب شده به ..... رسیده است،‌ تعداد کلمات کلیدی انتخاب شده از هر دسته‌خبری در ویژگی‌ها تقریبا با هم برابر می‌شود و انتظار داریم نتایجی بهتر از نتایج ذکر 
شده در جداول 
\ref{tbl:20ccm171tr} 
و
\ref{tbl:20ccm171ts}
مشاهده نماییم. جدول 
\ref{tbl:20ccm2000tr}
ماتریس درهم‌ریختگی را به برای داده‌های آموزشی و جدول 
\ref{tbl:20ccm2000ts}
این ماتریس را برای داده‌های آزمایشی، نمایش می‌دهند.
\\

\begin{table}[h]
\center
\caption{ماتریس درهم‌ریختگی حالت بیست کلاسه با داده‌های آموزشی و $\alpha = ....$}
\label{tbl:20ccm2000tr}
\begin{tabular}{c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c }
  شماره کلاس & ۱ & ۲ & ۳ & ۴ & ۵ & ۶ & ۷ & ۸ & ۹ & ۱۰ & ۱۱ & ۱۲ & ۱۳ & ۱۴ & ۱۵ & ۱۶ & ۱۷ & ۱۸ & ۱۹ & ۲۰ \\
\hline
\hline
تعداد واقعی موجود& 37  &  31  &  30  &  28  &  32  &  30  &  26  &  23  &  31  &  32  & 32   &  26  &  35  &  33  &  28  &  30  &  25  &  32  &  28  &   31 \\
درست مثبت& 11  &  12  &  4  &  14  &  3  &  0  &  1  &  20  &  8  &  0  &  1  &  12  & 3   &  12  &  0  &  15  &  10  &   12 &  13  &  7  \\
غلط مثبت&  2 &  3  & 5   &  8  &  4  &  10  &  5  &  347  &  6  & 0   &  1  &  8  & 0   &   12 &  0  &  7  &  12  &  0  &  8  &  4  \\

\end{tabular}
\end{table}

همان‌طور که مشاهده می‌شود..........

\begin{table}[h]
\center
\caption{ماتریس درهم‌ریختگی حالت بیست کلاسه با داده‌های آزمایشی و $\alpha = .....$}
\label{tbl:20ccm2000ts}
\begin{tabular}{c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c }
  شماره کلاس & ۱ & ۲ & ۳ & ۴ & ۵ & ۶ & ۷ & ۸ & ۹ & ۱۰ & ۱۱ & ۱۲ & ۱۳ & ۱۴ & ۱۵ & ۱۶ & ۱۷ & ۱۸ & ۱۹ & ۲۰ \\
\hline
\hline
تعداد واقعی موجود& 37  &  31  &  30  &  28  &  32  &  30  &  26  &  23  &  31  &  32  & 32   &  26  &  35  &  33  &  28  &  30  &  25  &  32  &  28  &   31 \\
درست مثبت& 11  &  12  &  4  &  14  &  3  &  0  &  1  &  20  &  8  &  0  &  1  &  12  & 3   &  12  &  0  &  15  &  10  &   12 &  13  &  7  \\
غلط مثبت&  2 &  3  & 5   &  8  &  4  &  10  &  5  &  347  &  6  & 0   &  1  &  8  & 0   &   12 &  0  &  7  &  12  &  0  &  8  &  4  \\

\end{tabular}
\end{table}

\section{بررسی استقلال شرطی ویژگی‌ها به شرط دانستن کلاس}

در مدل بیز ساده، فرض بر این است که تمامی ویژگی‌های استفاده شده، به شرط دانستن کلاس، از یک‌دیگر مستقل هستند و تنها استقلال شرطی موجود همین است. گراف معادل برای توزیع احتمالی با این ویژگی معادل شکل
\ref{fig:NBGraph}
می‌باشد.

\begin{figure}[h]
\center
\includegraphics[scale=0.5]{{/home/ahmad/Desktop/download.png}}
\caption{گراف معادل با توزیع احتمالی با شرط مفروض در مدل بیز ساده.}
\label{fig:NBGraph}
\end{figure}

فرض مذکور در این مدل را می‌توان با رابطه 
\ref{eq:NBIndCond}
نمایش داد. برای اثبات درستی یا نادرستی فرض،‌ به ازای تمام زوج‌ویژگی‌های موجود، باید دو طرف این رابطه را جداگانه محاسبه و سپس نتایج را باهم مقایسه کرد.


\begin{equation}
\label{eq:NBIndCond}
\forall i \neq j;  (X_i \perp X_j|C) \longleftrightarrow P(X_i,X_j| C) = P(X_i|C)\cdot P(X_j|C)  
\end{equation}

برای اثبات یا رد فرض، مقادیر $P(X_i|C)$ را به ازای تمام ویژگی‌ها باید محاسبه کرد که تمام توزیع‌های احتمالی شرطی  موجود بین یک ویژگی و یک دسته‌خبری قبلا محاسبه شده‌اند. برای محاسبه $P(X_i,X_j|C)$ نیز از ماتریس شاخص که در بخش پیش‌پردازش توضیح داده شد، استفاده می‌کنیم.


..................................

\section{تاثیر تعداد داده‌های آموزشی}















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vfill
\section{توضیحات}
\begin{itemize}
\item [*] سورس کد مربوط به پروژه در ضمیمه این گزارش ارسال شده است. همین‌طور این کد از
\href{https://github.com/ahmad-asadi/PGM/tree/master/BayesianNetwork}
{این لینک}
، قابل 
دریافت می‌باشد.
\item [*] آدرس لینک برای دریافت کد:
\LTR{
\url{https://github.com/ahmad-asadi/PGM/tree/master/BayesianNetwork}
}
\end{itemize}




\end{document} 
